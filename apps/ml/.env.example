# Pet Health ML Inference Service Configuration

# Service host (0.0.0.0 for Docker, localhost for local dev)
INFERENCE_HOST=0.0.0.0

# Service port
INFERENCE_PORT=5000

# Debug mode (true/false)
INFERENCE_DEBUG=false

# Model directory (relative or absolute path)
MODELS_DIR=models